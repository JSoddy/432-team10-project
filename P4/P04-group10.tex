\documentclass[11pt,twocolumn]{article}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}

%opening
\title{Motivating Data Structures With Unpredictable Timing}
\author{Group 10\\
\small{Tao Huang, Travis Wentz, James Corbett and James Soddy}}

\begin{document}

\maketitle

Protection from malicious attackers is easily understood as a general motivation
for creating data structures with unpredictable timing. More difficult to understand,
however, are the precise ways in which such structures are able to prevent adversaries
from exploiting a system. Our intention is to clearly illustrate, through
high level descriptions and illustrated examples, precisely the way in which attacks
can compromise standard systems, and how the protection offered by randomized
data structures works.

\section{Motivation}
What is the problem that this algorithm is trying to solve (note: you may want 
to borrow from your P-2 write-up).

\section{Algorithm}
Brief description of the main points of the algorithm.

\section{Analysis Problems}
These data structures have been analyzed, and details of their suitability and
efficiency have been given in the Bethea/Reiter paper\cite{Bethea09}. However,
making use of this information requires wading through fifteen pages of material such as:

\begin{minipage}{.5\textwidth}

$$\sum_s\sum_{h=1}^\infty\sum_{j=1}^{n_{i+1}} \left( \begin{matrix}
2^{-h}*Pr[S_i=s | I_i]*Pr[S_{i+1}=s' \\ \land dur(inv_{i+1})=d_{i+1}\  
|\  S_i=s \\ \land H_{i+1}=h \land O_{i+1}=v_j] \end{matrix} \right)$$
\hrule
$$\sum_s\sum_{h=1}^\infty\sum_{j=1}^{n_{i+1}} 
\left( \begin{matrix}2^{-h}*Pr[S_i=s | I_i]*
\\Pr[ dur(inv_{i+1})=d_{i+1} \\ \  | \  S_i=s \land H_{i+1}=h \land O_{i+1}=v_j] \end{matrix} \right)$$

\end{minipage}

It is our intention to present the details of the algorithm in such a way that
it is easily understood by anyone with a basic grasp of computing principles,
or even a motivated layperson.ds


\section{Discussion}

As the paper doesn't provide any comparison of performance of the proposed algorithm, readers may not have an idea about how much improvement introduced by this algorithm over some previous ones. Our first focus is on the performance comparison of the non-circular skip list based algorithm and the circular skip list based algorithm which is the proposed one in the paper. It's natural to compare two algorithms' safety performance which is the central issue in the paper. According to the paper, an adversary's attempts are basically based on probability calculation which may produce some hints at most expensive operations in the system. Using same data structure and invocations, we can simulate enough number of operations on the two algorithms in order to create two time distributions. Safety performance comparison can therefore be easily performed thanks to the fact that distributions can tell how much information revealed over a certain number of operations. For example, if a distribution is close to a uniform one, the adversary does not have a bigger chance to have a  better guess of which invocation is expensive. Along with the performance comparison, we may also perform running time comparison, because weighing pros and cons and finding a balance between them is a central part of any practical implementation of a new technique. Regarding the the above comparisons, we may use Fisher's information formula and running time as numerical summaries of the two most important aspects of algorithms than can be applied in the same scenario. Therefore it's possible to create  a cost function used to balance the pros and cons in catering to customers' demands. 

However, we may further our study of the algorithm by testing it on different data structures to see if serves  consistently well. But the challenging part of this testing would be how to define and choose different data structures as there are infinitely many different data structures. Some tentative structures may be the ones people in this area usually use for testing. Or in the worst case, randomly different data sets  may put on the algorithm to see if their distributions of times are similar. On the other hand, data size of different sizes will be tested to give us an idea of its asymptotic performance. 

If we are luck, we may be able to see some flaws or inconsistency in the algorithm after we run some simulations. Then we can discuss what was going on in the computation and if there's anything different we can do about it.


\bibliographystyle{plain}
\bibliography{bibfilename} 

\newpage
\onecolumn
\appendix
\section{Timeline}
(NOTE: it is good form to have a reference to this bibliography somewhere in 
the main body of your paper).

The following are a list of tasks that we need to accomplish in order to 
complete our project:

1. Program the algorithms in comparison and run some simulations.

2.Create the distributions of time to visualize their performances.

3.Run different date sets on the proposed algorithm and visualize the performance.

4.Make videos.

5.Discuss issues seen and possible solutions.

6. Formalize the project.

\paragraph{Structure the video.} We need to plan a detailed outline of our 
video.  We will meet together to do this.

\paragraph{Decide materials needed.}  Make sure you have everything you need in 
order to make this 4-5 minute video!

... and so forth.

The following table gives the timeline of how we plan to accomplish these tasks:

\begin{table}[h!]
\centering
\begin{tabular}{ |l | c | r|}
  \hline
  Date & Who? & Short Description \\
  \hline
  \hline
  11/07 & All & programming \\
  \hline
  11/11 & All & simulation \\
  
  \hline
  11/14 & All & performance comparison \\
  
  \hline
  11/16 & All & discussion of issues \\
  
  \hline
  11/21 & All & video and more improvement if possible \\
  
 
  \hline
  11/28 & All & Final fixing \\
  \hline
\end{tabular}
\end{table}

\pagebreak

\begin{thebibliography}{9}
	
\bibitem{Bethea09}
	Bethea, Darrell, and Michael K. Reiter,
	\emph{Data Structures with Unpredictable Timing}
	ESORICS,
	2009.
	
\end{thebibliography}

\end{document}
