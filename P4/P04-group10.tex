\documentclass[11pt,twocolumn]{article}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}

%opening
\title{Motivating Data Structures With Unpredictable Timing}
\author{Group 10\\
\small{Tao Huang, Travis Wentz, James Corbett and James Soddy}}

\begin{document}

\maketitle

Protection from malicious attackers is easily understood as a general motivation
for creating data structures with unpredictable timing. More difficult to understand,
however, are the precise ways in which such structures are able to prevent adversaries
from exploiting a system. Our intention is to clearly illustrate, through
high level descriptions and visual examples, precisely the way in which attacks
can compromise standard systems and how the protection offered by randomized
data structures works.

\section{Algorithm}
 At the core of this algorithm is the skip list, which is a specific type of linked list. This algorithm uses a special implementation called circular skip list so that there is truly no beginning nor end to the list. Using this data structure the algorithm chooses an "origin" at random that will serve as the starting point for insertions, deletions, and searches of the lists. A starting origin is chosen at random upon initialization of the circular skip list, then with every subsequent operation a new origin is chosen. The choosing of a new origin also causes a restructuring of the skip lists. The combination of these two factors results in the timing unpredictably of the algorithm. With each call, the adversary itself alters the skip list making any assumptions as to the nature of the lists extremely difficult.
  
\section{Analysis Problems}
These data structures have been analyzed, and details of their suitability and
efficiency have been given in the Bethea/Reiter paper\cite{Bethea09}. However,
making use of this information requires wading through fifteen pages of material such as:

\begin{minipage}{.5\textwidth}

$$\sum_s\sum_{h=1}^\infty\sum_{j=1}^{n_{i+1}} \left( \begin{matrix}
2^{-h}*Pr[S_i=s | I_i]*Pr[S_{i+1}=s' \\ \land dur(inv_{i+1})=d_{i+1}\  
|\  S_i=s \\ \land H_{i+1}=h \land O_{i+1}=v_j] \end{matrix} \right)$$
\hrule
$$\sum_s\sum_{h=1}^\infty\sum_{j=1}^{n_{i+1}} 
\left( \begin{matrix}2^{-h}*Pr[S_i=s | I_i]*
\\Pr[ dur(inv_{i+1})=d_{i+1} \\ \  | \  S_i=s \land H_{i+1}=h \land O_{i+1}=v_j] \end{matrix} \right)$$

\end{minipage}

It is our intention to present the details of the algorithm in such a way that
it is easily understood by anyone with a basic grasp of computing principles,
as well as accessible to a motivated layperson.


\section{Discussion}

As the paper does not provide any comparison of performance of the proposed algorithm, readers may not have an idea of how much this improves upon previous algorithms. Our first focus will be a performance comparison of the non-circular skip list based algorithm to the circular skip list based algorithm which is proposed in the source paper. Comparing the safety performance of the two algorithms is natural as safety performance is the central issue in the source paper. In simple terms, an adversary's attempts to attack a system are based on probability calculation. Discovering patterns may produce some hints at most expensive operations in the system. Using same data structure and invocations, we can simulate a sufficient number of operations on the two algorithms in order to create two time distributions. Safety performance comparison can therefore be easily performed, thanks to the fact that distributions can show how much information is revealed over a certain number of operations. For example, if a distribution is close to uniform, the adversary does not have a better chance at guessing which invocation is expensive. 

Because it is important to find a balance between performance in terms of security and performance in terms of speed, we may also perform run time comparison. In such comparisons, we may use Fisher's information formula and run time as numerical summaries of the two most important aspects of algorithms than can be applied in the same scenario. This may make it possible to create a cost function which can be used to balance speed and security in catering to customers' demands. 

Additionally, we may further our study of the algorithm by testing it on different data structures. The challenging part of this would be choosing suitable data structures, as there are infinitely many data structures. Some tentative structures may be the ones people in this area normally used for testing. Or in the worst case, randomly chosen data sets may put on the algorithm to see if their distribution times are similar. And finally, data of different sizes will be tested to give us an idea of its asymptotic performance. 

It may be that we see some flaws or inconsistencies in the algorithm after we run some simulations. In that case, we can discuss what is going on in the computation and if there is anything different we can do.


\bibliographystyle{plain}
\bibliography{bibfilename} 

\newpage
\onecolumn
\appendix
\section{Timeline}

The following table gives the timeline of how we plan to accomplish these tasks:

\begin{table}[h!]
\centering
\begin{tabular}{ |l | c | r|}
  \hline
  Date & Who? & Short Description \\
  \hline
  \hline
  11/07 & All & programming \\
  \hline
  11/11 & All & simulation \\
  
  \hline
  11/14 & All & performance comparison \\
  
  \hline
  11/16 & All & discussion of issues \\
  
  \hline
  11/21 & All & video and more improvement if possible \\
  
 
  \hline
  11/28 & All & Final fixing \\
  \hline
\end{tabular}
\end{table}

\pagebreak

\begin{thebibliography}{9}
	
\bibitem{Bethea09}
	Bethea, Darrell, and Michael K. Reiter,
	\emph{Data Structures with Unpredictable Timing}
	ESORICS,
	2009.
	
\end{thebibliography}

\end{document}


