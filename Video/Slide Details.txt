


Slide 17:
	We had some question as to the actual costs and benefits of using the randomized structure in practice. We
wanted to have some concrete numbers to show, to show precisely why such a structure was important.

	Some aspects of the algorithm and the paper were confusing. We thought that by implementing it, not only would
it help us to clarify those elements for others who don't want to take time studying the paper, but it would
greatly improve our understanding of how the structure works as well.

Slide 18:
	We had some odd results that we didn't expect. For both the standard and the circular lists, some set sizes
seemed inclined to faster searches than others, even though they might be dozens of times larger. We assume that
is mostly due to random variation and implementation quirks. For larger data sets the timing smoothed out to look
very much like what we expected.

Slide 19/20:
	Our results showed that our circular implementation performed around three times slower under normal operation
than the standard skip-list across all sizes. However, at large data sizes, and when under attack, the circular
list performed hundreds or even thousands of times faster.

Slide 21/22/23:
	Our results are very close to what we expected. Although the costs of maintaining the circular list were slightly
higher than we expected, all of the operations still operated clearly within logarithmic time. We think it is likely
that with a little more care, and some code optimization, we could reduce the cost of most operations under the
circular list by around 1/2, putting them more in line with our expectations. We were pleased by
how well the structure held up under attack, and how stark the difference was between the standard and circular lists.

	Many of the small operational details were much more challenging to implement than was initially expected. Some
of the differences between the standard and circular lists, which we had assumed would just require minor tweaks to
put in place, in fact ended up necessitating a complete rewrite of many of our methods.

	We had hoped to run operations on sets with membership in the 100 million range, and from a computation time
standpoint this should have been possible, but it turns out memory constrainsts put a firm limit on the size of our
work at about 1/10th of that.

Slide 24/25 (There is some ambiguity over in which location these should go) :
The logic in the paper for searching was somewhat confusing, and had some elements which didn't work make sense
with some of the changes we had made. We used a search logic with fewer logical elements, because it was simpler
and made good sense with our structure.

We chose not to implement the index field as described in the paper. Rather than operating by direct index, we
always chose our random elements as an offset from our current head.

	In the algorithm proposal, they discussed using an origin which was separate from any
of the actual data elements. In our implementation, we have head element which is just a regular data element
chosen at random each time. The reason for this is that there was a great deal of additional complexity
in keeping the origin accurate with each operation. The drawback is that it perhaps increased the cost
of all find/add/remove operations by about 50% because we had to resize both the head element and the previous
head element on each operation. This also leaks some possibly exploitable information. Those concerns were unimportant
for our purposes.

Additionally, we made slight changes to how elements are resized on operations. (This is due to our head, in fact.) In
the paper they chose a new height before attempting to insert and then either added the element at the new height or, if
the element already existed, resized the origin target to the chosen height. Because were uesed an actual element as head,
we had to resize an element even if we successfully inserted a new element.

Slide 29:

It would have been nice to implement a fully random regular skip list, and do analysis on timing to see if we
could find exploitable information. This sort of analysis proved to be beyond the scope of our project.

Slide 30:

It would be trivial to apply the same principals to a multiset data type.

A similar implementation in a binary search tree could be used to prevent the distribution from becoming degenerate
when subject to an unfavorable series of insertions and deletions. 

???